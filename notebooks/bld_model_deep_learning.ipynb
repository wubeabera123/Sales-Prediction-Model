{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import numpy as np  # Make sure numpy is imported\n",
    "import tensorflow as tf\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='../logs/predict_store_sales_with_deep_learning.log',\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "  \n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../scripts')))\n",
    "from load_data import Load_Data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from ../data/combined_cleaned_data.csv\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Load cleaned store data\")\n",
    "\n",
    "# Create an instance of CSVReader\n",
    "df = Load_Data('../data/combined_cleaned_data.csv')\n",
    "\n",
    "# Load the data\n",
    "df.load_data()\n",
    "\n",
    "# Get the loaded data\n",
    "df = df.get_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Store                          int64\n",
      "CompetitionDistance          float64\n",
      "CompetitionOpenSinceMonth    float64\n",
      "CompetitionOpenSinceYear     float64\n",
      "Promo2                         int64\n",
      "Promo2SinceWeek              float64\n",
      "Promo2SinceYear              float64\n",
      "PromoInterval                 object\n",
      "DayOfWeek                      int64\n",
      "Date                          object\n",
      "Sales                        float64\n",
      "Customers                      int64\n",
      "Open                           int64\n",
      "Promo                          int64\n",
      "SchoolHoliday                  int64\n",
      "StoreType_b                     bool\n",
      "StoreType_c                     bool\n",
      "StoreType_d                     bool\n",
      "Assortment_b                    bool\n",
      "Assortment_c                    bool\n",
      "StateHoliday_a                  bool\n",
      "StateHoliday_b                  bool\n",
      "StateHoliday_c                  bool\n",
      "IsWeekend                      int64\n",
      "MonthDay                       int64\n",
      "IsBeginningOfMonth             int64\n",
      "IsMidMonth                     int64\n",
      "IsEndOfMonth                   int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wubeshet.abera\\AppData\\Local\\Temp\\ipykernel_2844\\2317894657.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Sales'] = data['Sales'].diff()\n",
      "C:\\Users\\wubeshet.abera\\AppData\\Local\\Temp\\ipykernel_2844\\2317894657.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['target'] = data['Sales'].shift(-1)\n",
      "C:\\Users\\wubeshet.abera\\AppData\\Local\\Temp\\ipykernel_2844\\2317894657.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.dropna(inplace=True)  # Remove rows with NaN values\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Index(['PromoInterval'], dtype='object')] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m data\u001b[38;5;241m.\u001b[39mdropna(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Remove rows with NaN values\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# One-hot encode the 'PromoInterval' column if necessary\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mPromoInterval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Scale the data\u001b[39;00m\n\u001b[0;32m     34\u001b[0m scaler \u001b[38;5;241m=\u001b[39m MinMaxScaler(feature_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\wubeshet.abera\\Projects\\Ten-Academy\\Sales-Prediction-Model\\venv\\Lib\\site-packages\\pandas\\core\\reshape\\encoding.py:169\u001b[0m, in \u001b[0;36mget_dummies\u001b[1;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be a list-like for parameter `columns`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m     data_to_encode \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# validate prefixes and separator to avoid silently dropping cols\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_len\u001b[39m(item, name: \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\wubeshet.abera\\Projects\\Ten-Academy\\Sales-Prediction-Model\\venv\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\wubeshet.abera\\Projects\\Ten-Academy\\Sales-Prediction-Model\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wubeshet.abera\\Projects\\Ten-Academy\\Sales-Prediction-Model\\venv\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6249\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nmissing:\n\u001b[0;32m   6248\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m nmissing \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(indexer):\n\u001b[1;32m-> 6249\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m     not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m   6252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Index(['PromoInterval'], dtype='object')] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load your DataFrame\n",
    "# df = pd.read_csv('your_data.csv')  # Example of loading your data\n",
    "\n",
    "# Ensure the 'Date' column is in datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Prepare the data\n",
    "# Prepare the data with all relevant features\n",
    "data = df[['Sales', 'CompetitionDistance', 'Promo2', 'DayOfWeek', 'Store', \n",
    "            'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', \n",
    "            'Promo2SinceWeek', 'Promo2SinceYear', 'IsWeekend', \n",
    "            'MonthDay', 'SchoolHoliday', 'StoreType_b', \n",
    "            'StoreType_c', 'StoreType_d', 'Assortment_b', \n",
    "            'Assortment_c', 'StateHoliday_a', 'StateHoliday_b', \n",
    "            'StateHoliday_c', 'IsBeginningOfMonth', 'IsMidMonth', \n",
    "            'IsEndOfMonth']]\n",
    "\n",
    "# Create target variable with differencing\n",
    "data['Sales'] = data['Sales'].diff()\n",
    "data['target'] = data['Sales'].shift(-1)\n",
    "data.dropna(inplace=True)  # Remove rows with NaN values\n",
    "\n",
    "# One-hot encode the 'PromoInterval' column if necessary\n",
    "data = pd.get_dummies(data, columns=['PromoInterval'], drop_first=True)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare the data for LSTM\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        X.append(data[i:(i + time_step), :-1])  # All columns except target\n",
    "        y.append(data[i + time_step, -1])  # Target column\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 10  # Use the last 10 days to predict the next day\n",
    "X, y = create_dataset(scaled_data, time_step)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Reshape features for LSTM\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions if necessary\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "# Evaluate your model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f'Test RMSE: {rmse:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
