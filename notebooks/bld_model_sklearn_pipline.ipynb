{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import logging\n",
    "import numpy as np  # Make sure numpy is imported\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(filename='../logs/predict_store_sales.log',\n",
    "                    level=logging.INFO,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "  \n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../scripts')))\n",
    "from load_data import Load_Data\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data sets(train and store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully loaded from ../data/combined_store_data.csv\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Load cleaned store data\")\n",
    "\n",
    "# Create an instance of CSVReader\n",
    "df = Load_Data('../data/combined_store_data.csv')\n",
    "\n",
    "# Load the data\n",
    "df.load_data()\n",
    "\n",
    "# Get the loaded data\n",
    "df = df.get_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Store StoreType Assortment  CompetitionDistance  CompetitionOpenSinceMonth  \\\n",
      "0      1         c          a               1270.0                        9.0   \n",
      "1      1         c          a               1270.0                        9.0   \n",
      "2      1         c          a               1270.0                        9.0   \n",
      "3      1         c          a               1270.0                        9.0   \n",
      "4      1         c          a               1270.0                        9.0   \n",
      "\n",
      "   CompetitionOpenSinceYear  Promo2  Promo2SinceWeek  Promo2SinceYear  \\\n",
      "0                    2008.0       0              NaN              NaN   \n",
      "1                    2008.0       0              NaN              NaN   \n",
      "2                    2008.0       0              NaN              NaN   \n",
      "3                    2008.0       0              NaN              NaN   \n",
      "4                    2008.0       0              NaN              NaN   \n",
      "\n",
      "  PromoInterval  DayOfWeek        Date  Sales  Customers  Open  Promo  \\\n",
      "0           NaN          5  2015-07-31   5263        555     1      1   \n",
      "1           NaN          4  2015-07-30   5020        546     1      1   \n",
      "2           NaN          3  2015-07-29   4782        523     1      1   \n",
      "3           NaN          2  2015-07-28   5011        560     1      1   \n",
      "4           NaN          1  2015-07-27   6102        612     1      1   \n",
      "\n",
      "  StateHoliday  SchoolHoliday  \n",
      "0            0              1  \n",
      "1            0              1  \n",
      "2            0              1  \n",
      "3            0              1  \n",
      "4            0              1  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Categorical Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Store  CompetitionDistance  CompetitionOpenSinceMonth  \\\n",
      "0      1               1270.0                        9.0   \n",
      "1      1               1270.0                        9.0   \n",
      "2      1               1270.0                        9.0   \n",
      "3      1               1270.0                        9.0   \n",
      "4      1               1270.0                        9.0   \n",
      "\n",
      "   CompetitionOpenSinceYear  Promo2  Promo2SinceWeek  Promo2SinceYear  \\\n",
      "0                    2008.0       0              NaN              NaN   \n",
      "1                    2008.0       0              NaN              NaN   \n",
      "2                    2008.0       0              NaN              NaN   \n",
      "3                    2008.0       0              NaN              NaN   \n",
      "4                    2008.0       0              NaN              NaN   \n",
      "\n",
      "  PromoInterval  DayOfWeek        Date  ...  Promo  SchoolHoliday  \\\n",
      "0           NaN          5  2015-07-31  ...      1              1   \n",
      "1           NaN          4  2015-07-30  ...      1              1   \n",
      "2           NaN          3  2015-07-29  ...      1              1   \n",
      "3           NaN          2  2015-07-28  ...      1              1   \n",
      "4           NaN          1  2015-07-27  ...      1              1   \n",
      "\n",
      "   StoreType_b  StoreType_c  StoreType_d  Assortment_b  Assortment_c  \\\n",
      "0        False         True        False         False         False   \n",
      "1        False         True        False         False         False   \n",
      "2        False         True        False         False         False   \n",
      "3        False         True        False         False         False   \n",
      "4        False         True        False         False         False   \n",
      "\n",
      "   StateHoliday_a  StateHoliday_b  StateHoliday_c  \n",
      "0           False           False           False  \n",
      "1           False           False           False  \n",
      "2           False           False           False  \n",
      "3           False           False           False  \n",
      "4           False           False           False  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "logging.info('Starting preprocessing with updated dataframe')\n",
    "# One-hot encode StoreType, Assortment, and StateHoliday\n",
    "df = pd.get_dummies(df, columns=['StoreType', 'Assortment', 'StateHoliday'], drop_first=True)\n",
    "logging.info('One-hot encoded StoreType, Assortment, and StateHoliday columns')\n",
    "\n",
    "# Check result\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Handle Competition and Promo2 Features & missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Promo2SinceWeek  Promo2SinceYear PromoInterval\n",
      "0              0.0              0.0          None\n",
      "1              0.0              0.0          None\n",
      "2              0.0              0.0          None\n",
      "3              0.0              0.0          None\n",
      "4              0.0              0.0          None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wubeshet.abera\\AppData\\Local\\Temp\\ipykernel_8544\\237365783.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Promo2SinceWeek'].fillna(0, inplace=True)\n",
      "C:\\Users\\wubeshet.abera\\AppData\\Local\\Temp\\ipykernel_8544\\237365783.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Promo2SinceYear'].fillna(0, inplace=True)\n",
      "C:\\Users\\wubeshet.abera\\AppData\\Local\\Temp\\ipykernel_8544\\237365783.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['PromoInterval'].fillna('None', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "logging.info('Handling missing values for Promo2SinceWeek, Promo2SinceYear, and PromoInterval')\n",
    "\n",
    "# Fill missing values for Promo2SinceWeek and Promo2SinceYear with 0 (no promotion)\n",
    "df['Promo2SinceWeek'].fillna(0, inplace=True)\n",
    "df['Promo2SinceYear'].fillna(0, inplace=True)\n",
    "\n",
    "# Fill missing PromoInterval with 'None'\n",
    "df['PromoInterval'].fillna('None', inplace=True)\n",
    "logging.info('Filled missing values in Promo2SinceWeek, Promo2SinceYear, and PromoInterval')\n",
    "\n",
    "# Check result\n",
    "print(df[['Promo2SinceWeek', 'Promo2SinceYear', 'PromoInterval']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction from Date Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "logging.info('Extracting features from the Date column')\n",
    "\n",
    "# Convert 'Date' to datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Extract weekday\n",
    "df['DayOfWeek'] = df['Date'].dt.weekday + 1  # Monday=1, Sunday=7\n",
    "\n",
    "# Extract is_weekend (1 if weekend, 0 if weekday)\n",
    "df['IsWeekend'] = df['DayOfWeek'].apply(lambda x: 1 if x >= 6 else 0)\n",
    "\n",
    "# Check if it's beginning, mid, or end of the month\n",
    "df['MonthDay'] = df['Date'].dt.day\n",
    "df['IsBeginningOfMonth'] = df['MonthDay'].apply(lambda x: 1 if x <= 10 else 0)\n",
    "df['IsMidMonth'] = df['MonthDay'].apply(lambda x: 1 if 10 < x <= 20 else 0)\n",
    "df['IsEndOfMonth'] = df['MonthDay'].apply(lambda x: 1 if x > 20 else 0)\n",
    "\n",
    "logging.info('Extracted day of week, weekend indicator, and month-day-related features')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Sales  CompetitionDistance  Promo2SinceWeek  Promo2SinceYear\n",
      "0 -0.132683            -0.539198        -0.760097        -1.001128\n",
      "1 -0.195801            -0.539198        -0.760097        -1.001128\n",
      "2 -0.257620            -0.539198        -0.760097        -1.001128\n",
      "3 -0.198139            -0.539198        -0.760097        -1.001128\n",
      "4  0.085244            -0.539198        -0.760097        -1.001128\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['../saved_models/my_scaler.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "\n",
    "logging.info('Starting to scale numeric features')\n",
    "\n",
    "# Define features to scale\n",
    "features_to_scale = ['Sales', 'CompetitionDistance', 'Promo2SinceWeek', 'Promo2SinceYear']\n",
    "\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "df[features_to_scale] = scaler.fit_transform(df[features_to_scale])\n",
    "\n",
    "logging.info('Scaled features: Sales, CompetitionDistance, Promo2SinceWeek, and Promo2SinceYear')\n",
    "\n",
    "# Check result\n",
    "print(df[features_to_scale].head())\n",
    "\n",
    "joblib.dump(scaler, '../saved_models/my_scaler.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Store  CompetitionDistance  CompetitionOpenSinceMonth  \\\n",
      "0      1            -0.539198                        9.0   \n",
      "1      1            -0.539198                        9.0   \n",
      "2      1            -0.539198                        9.0   \n",
      "3      1            -0.539198                        9.0   \n",
      "4      1            -0.539198                        9.0   \n",
      "\n",
      "   CompetitionOpenSinceYear  Promo2  Promo2SinceWeek  Promo2SinceYear  \\\n",
      "0                    2008.0       0        -0.760097        -1.001128   \n",
      "1                    2008.0       0        -0.760097        -1.001128   \n",
      "2                    2008.0       0        -0.760097        -1.001128   \n",
      "3                    2008.0       0        -0.760097        -1.001128   \n",
      "4                    2008.0       0        -0.760097        -1.001128   \n",
      "\n",
      "  PromoInterval  DayOfWeek       Date  ...  Assortment_b  Assortment_c  \\\n",
      "0          None          5 2015-07-31  ...         False         False   \n",
      "1          None          4 2015-07-30  ...         False         False   \n",
      "2          None          3 2015-07-29  ...         False         False   \n",
      "3          None          2 2015-07-28  ...         False         False   \n",
      "4          None          1 2015-07-27  ...         False         False   \n",
      "\n",
      "   StateHoliday_a  StateHoliday_b  StateHoliday_c  IsWeekend  MonthDay  \\\n",
      "0           False           False           False          0        31   \n",
      "1           False           False           False          0        30   \n",
      "2           False           False           False          0        29   \n",
      "3           False           False           False          0        28   \n",
      "4           False           False           False          0        27   \n",
      "\n",
      "   IsBeginningOfMonth  IsMidMonth  IsEndOfMonth  \n",
      "0                   0           0             1  \n",
      "1                   0           0             1  \n",
      "2                   0           0             1  \n",
      "3                   0           0             1  \n",
      "4                   0           0             1  \n",
      "\n",
      "[5 rows x 28 columns]\n"
     ]
    }
   ],
   "source": [
    "logging.info('Preprocessing completed and dataset is ready for modeling')\n",
    "\n",
    "# Display the first few rows to verify\n",
    "df.to_csv('../data/combined_cleaned_data.csv', index=False)\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Models with Sklearn Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "# Drop 'Sales' and 'Date' columns, 'Sales' is the target variable\n",
    "X = df.drop(['Sales', 'Date'], axis=1)\n",
    "y = df['Sales']\n",
    "\n",
    "# Check for missing values in X\n",
    "logging.info(f\"Missing values in X before preprocessing: \\n{X.isnull().sum()}\")\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Log the shape of the dataset\n",
    "logging.info(f\"Training data shape: {X_train.shape}\")\n",
    "logging.info(f\"Testing data shape: {X_test.shape}\")\n",
    "\n",
    "# Define numeric and categorical columns\n",
    "numeric_features = ['Store', 'CompetitionDistance', 'CompetitionOpenSinceMonth',\n",
    "                    'CompetitionOpenSinceYear', 'Promo2', 'Promo2SinceWeek', \n",
    "                    'Promo2SinceYear', 'DayOfWeek', 'MonthDay', \n",
    "                    'IsWeekend', 'IsBeginningOfMonth', 'IsMidMonth', 'IsEndOfMonth']\n",
    "\n",
    "categorical_features = ['PromoInterval', 'Assortment_b', 'Assortment_c', \n",
    "                        'StateHoliday_a', 'StateHoliday_b', 'StateHoliday_c']\n",
    "\n",
    "# Define the preprocessing pipeline: scaling for numeric, one-hot encoding for categorical\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the RandomForestRegressor model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create a pipeline that applies the preprocessor and then fits the model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Log the pipeline setup\n",
    "logging.info(\"Pipeline created with StandardScaler, OneHotEncoder, and RandomForestRegressor.\")\n",
    "\n",
    "# Fit the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Log after fitting the model\n",
    "logging.info(\"Model training completed.\")\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# Log prediction completion\n",
    "logging.info(\"Model prediction completed.\")\n",
    "\n",
    "# Evaluate the model performance (using RMSE)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "logging.info(f\"Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Evaluate the model performance (using MAE)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "logging.info(f\"Test MAE: {mae:.4f}\")\n",
    "\n",
    "# Extract feature importances (note: this will work only if the model supports it)\n",
    "model = pipeline.named_steps['model']  # Get the trained model from the pipeline\n",
    "if hasattr(model, 'feature_importances_'):\n",
    "    feature_importances = model.feature_importances_\n",
    "    # Get feature names after preprocessing (this includes one-hot encoded feature names)\n",
    "    feature_names = numeric_features + list(pipeline.named_steps['preprocessor']\n",
    "                                            .named_transformers_['cat']\n",
    "                                            .get_feature_names_out(categorical_features))\n",
    "    \n",
    "    important_features = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': feature_importances\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    logging.info(f\"Feature importances: {important_features}\")\n",
    "else:\n",
    "    logging.info(\"The model does not support feature importance extraction.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Prediction Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Visualize Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'important_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Sort feature importances\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m important_features \u001b[38;5;241m=\u001b[39m \u001b[43mimportant_features\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImportance\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Plot the top features\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'important_features' is not defined"
     ]
    }
   ],
   "source": [
    "# Sort feature importances\n",
    "important_features = important_features.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Plot the top features\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=important_features.head(10))\n",
    "plt.title('Top 10 Most Important Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Interval for Predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Bootstrapping for Confidence Interval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_confidence_interval(pipeline, X_test, n_iterations=100, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Estimate prediction confidence intervals using bootstrapping.\n",
    "    Args:\n",
    "    - pipeline: the fitted model pipeline.\n",
    "    - X_test: test features.\n",
    "    - n_iterations: the number of bootstrap samples.\n",
    "    - alpha: significance level (0.05 for 95% CI).\n",
    "    \n",
    "    Returns:\n",
    "    - lower and upper bounds of the confidence intervals for each prediction.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    \n",
    "    # Perform bootstrapping\n",
    "    for i in range(n_iterations):\n",
    "        # Sample X_test with replacement\n",
    "        indices = np.random.choice(np.arange(X_test.shape[0]), size=X_test.shape[0], replace=True)\n",
    "        X_sample = X_test.iloc[indices]\n",
    "        \n",
    "        # Get predictions for the sample\n",
    "        y_sample_pred = pipeline.predict(X_sample)\n",
    "        predictions.append(y_sample_pred)\n",
    "    \n",
    "    # Convert to NumPy array\n",
    "    predictions = np.array(predictions)\n",
    "    \n",
    "    # Calculate the confidence intervals\n",
    "    lower_bound = np.percentile(predictions, 100 * alpha / 2, axis=0)\n",
    "    upper_bound = np.percentile(predictions, 100 * (1 - alpha / 2), axis=0)\n",
    "    \n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# Estimate confidence intervals\n",
    "lower_ci, upper_ci = bootstrap_confidence_interval(pipeline, X_test)\n",
    "\n",
    "# Display results for the first 5 predictions\n",
    "for i in range(5):\n",
    "    print(f\"Prediction: {y_pred[i]:.4f}, CI: [{lower_ci[i]:.4f}, {upper_ci[i]:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as ../saved_models/model-22-09-2024-14-59-02-071.pkl\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "# Generate timestamp in the format dd-mm-yyyy-hh-mm-ss-ms\n",
    "timestamp = datetime.datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S-%f\")[:-3]\n",
    "\n",
    "# Define the folder where you want to save the models\n",
    "save_folder = '../saved_models'  # Adjust the path as needed\n",
    "\n",
    "# Ensure the folder exists\n",
    "if not os.path.exists(save_folder):\n",
    "    os.makedirs(save_folder)\n",
    "\n",
    "# Define the full path for the file\n",
    "model_filename = f\"{save_folder}/model-{timestamp}.pkl\"\n",
    "\n",
    "# Assuming the model is stored in a variable called 'pipeline'\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(pipeline, file)\n",
    "\n",
    "print(f\"Model saved as {model_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load your DataFrame\n",
    "# df = pd.read_csv('your_data.csv')  # Example of loading your data\n",
    "\n",
    "# Ensure the 'Date' column is in datetime format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Prepare the data\n",
    "# Prepare the data with all relevant features\n",
    "data = df[['Sales', 'CompetitionDistance', 'Promo2', 'DayOfWeek', 'Store', \n",
    "            'CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear', \n",
    "            'Promo2SinceWeek', 'Promo2SinceYear', 'IsWeekend', \n",
    "            'MonthDay', 'SchoolHoliday', 'StoreType_b', \n",
    "            'StoreType_c', 'StoreType_d', 'Assortment_b', \n",
    "            'Assortment_c', 'StateHoliday_a', 'StateHoliday_b', \n",
    "            'StateHoliday_c', 'IsBeginningOfMonth', 'IsMidMonth', \n",
    "            'IsEndOfMonth']]\n",
    "\n",
    "# Create target variable with differencing\n",
    "data['Sales'] = data['Sales'].diff()\n",
    "data['target'] = data['Sales'].shift(-1)\n",
    "data.dropna(inplace=True)  # Remove rows with NaN values\n",
    "\n",
    "# One-hot encode the 'PromoInterval' column if necessary\n",
    "data = pd.get_dummies(data, columns=['PromoInterval'], drop_first=True)\n",
    "\n",
    "# Scale the data\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Prepare the data for LSTM\n",
    "def create_dataset(data, time_step=1):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - time_step - 1):\n",
    "        X.append(data[i:(i + time_step), :-1])  # All columns except target\n",
    "        y.append(data[i + time_step, -1])  # Target column\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "time_step = 10  # Use the last 10 days to predict the next day\n",
    "X, y = create_dataset(scaled_data, time_step)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Reshape features for LSTM\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Inverse transform the predictions if necessary\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "\n",
    "# Evaluate your model\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "print(f'Test RMSE: {rmse:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
